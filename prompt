from config import LLAMA_API
import numpy as np
from config import TAGS_LIST, LLAMA_API, CATEGORY_SUBCATEGORY_LIST
import re
import json
import pandas as pd
import os
from src.utils import timing_decorator
main.py:


from src.history_manager import History
from src.llamarequest import llm_api
from src.poi_filter import get_poi_data
from src.get_top_candidates import find_top_candidates
from src.get_location_advice import get_location_advice


def get_llm_response(user_prompt, user_context):
    llm_response = llm_api(user_prompt, user_context)
    print("DEBUG: LLM Response:", llm_response)

    # Check if clarification is needed
    clarification_needed = llm_response.get("clarification", None)

    if clarification_needed:
        # Clarification is a string, we can check if it's asking for clarification
        if isinstance(clarification_needed, str):
            print("Clarification Needed:", clarification_needed)
            additional_input = input("Provide clarification: ")
            user_prompt += " " + additional_input
            llm_response = llm_api(user_prompt, user_context)
        else:
            clarification_question = clarification_needed.get("question", "")
            if clarification_question:
                print("Clarification Needed:", clarification_question)
                additional_input = input("Provide clarification: ")
                user_prompt += " " + additional_input
                llm_response = llm_api(user_prompt, user_context)

    if 'error' in llm_response:
        print("Error:", llm_response['error'])
        return None

    return llm_response


def poi_process(llm_response, latitude, longitude, search_radius):
    search_categories = llm_response.get("categories", [])
    if not search_categories:
        print("Failed to extract valid categories from LLM response.")
        return []

    candidates = get_poi_data(
        latitude, longitude, search_radius, search_categories)
    if not candidates:
        print("No POIs found based on your criteria.")
    return candidates


def candidates_process(candidates, latitude, longitude, search_radius, num_candidates):
    candidate_results = find_top_candidates(
        candidates, latitude, longitude, search_radius, num_candidates)
    return {"default": candidate_results} if not isinstance(candidate_results, dict) else candidate_results


def get_location_advice_for_prompt(top_candidates, user_prompt, previous_messages, latitude, longitude, search_radius):
    try:
        print("DEBUG: Top Candidates before giving location advice:", top_candidates)

        location_advice = get_location_advice(
            user_prompt, previous_messages, top_candidates, latitude, longitude, search_radius)
        return location_advice
    except Exception as e:
        print(f"Error during location advice processing: {e}")
        return {}


def save_conversation_history(history_manager, conversation_id, user_prompt, response_text, top_candidates, continuation):
    try:
        history_manager.save_history(
            conversation_id,
            user_prompt,
            response_text,
            top_candidates if not continuation else top_candidates  # corrected this line
        )
    except Exception as e:
        print(f"Error saving history: {e}")


def main():
    history_manager = History()
    conversation_id = input("Enter conversation ID: ")

    previous_messages = history_manager.get_conversation(conversation_id)
    for msg in previous_messages:
        print(f"User: {msg['user']}")
        print(f"Bot: {msg['response']}")

    latitude = 41.064108
    longitude = 29.031473
    search_radius = 2000
    num_candidates = 2

    while True:
        stored_top_candidates = history_manager.get_top_candidates(
            conversation_id)
        top_candidates = {}

        if not stored_top_candidates:
            user_prompt = input("Enter your prompt (or type 'exit' to quit): ")
            if user_prompt.lower() == 'exit':
                break

            user_context = [
                f"{msg['user']} {msg['response']}" for msg in previous_messages]

            llm_response = get_llm_response(user_prompt, user_context)
            if llm_response is None:
                continue

            candidates = poi_process(
                llm_response, latitude, longitude, search_radius)
            if not candidates:
                continue

            top_candidates = candidates_process(
                candidates, latitude, longitude, search_radius, num_candidates)

        else:
            top_candidates = stored_top_candidates
            user_prompt = input("Enter your prompt (or type 'exit' to quit): ")
            if user_prompt.lower() == 'exit':
                break

        location_advice = get_location_advice_for_prompt(
            top_candidates, user_prompt, previous_messages, latitude, longitude, search_radius)
        response_text = location_advice.get(
            "response", "No response received.")
        print("\nLocation Advice:", response_text)

        continuation = location_advice.get("continuation", False)
        if isinstance(continuation, str):
            continuation = continuation.lower() == "true"

        save_conversation_history(history_manager, conversation_id,
                                  user_prompt, response_text, top_candidates, continuation)


if __name__ == "__main__":
    main()


llamarequest.py:


@timing_decorator
def retrieve_tags():
    """
    Retrieves tags and subcategories from CSV files and returns them as formatted strings.
    """
    tags_string = "None"
    subcategory_string = "None"

    # Retrieve tags
    if os.path.exists(TAGS_LIST):
        tags_df = pd.read_csv(TAGS_LIST)
        if 'tags' in tags_df.columns:
            tags_list = tags_df['tags'].dropna().tolist()
            tags_string = ", ".join(tags_list) if tags_list else "None"

    # Retrieve subcategories
    if os.path.exists(CATEGORY_SUBCATEGORY_LIST):
        subcategory_df = pd.read_csv(CATEGORY_SUBCATEGORY_LIST)
        if 'category' in subcategory_df.columns and 'subcategory' in subcategory_df.columns:
            # Group by 'category' and aggregate the subcategories into a comma-separated string
            grouped = subcategory_df.groupby('category')['subcategory'].apply(
                lambda x: ",".join(x)).reset_index()

            # Format the string as Category: subcat,subcat,...
            subcategory_string = "\n".join(
                [f"{row['category']}: {row['subcategory']}" for _, row in grouped.iterrows()])
            subcategory_string = subcategory_string if subcategory_string else "None"

    return tags_string, subcategory_string


@timing_decorator
def extract_json(response):
    response_data = response.json()
    print("response:", response_data)

    content = response_data['choices'][0]['message']['content']

    try:
        result = json.loads(content)
        print("✅ Direct JSON parsing successful.")
        return result
    except json.JSONDecodeError:
        print("❌ Direct JSON parsing failed. Trying regex extraction...")

    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        json_str = match.group(0).strip()
        try:
            result = json.loads(json_str)
            print("✅ Regex JSON extraction successful.")
            return result
        except json.JSONDecodeError:
            print("❌ Regex JSON extraction also failed.")

    print("❌ No valid JSON found.")
    return None


@timing_decorator
def llm_api(prompt, user_context=None):
    """
    LLM function that extracts subcategories and descriptive tags.
    If multiple subcategories are found, the LLM itself will ask for more detail.
    """
    existing_tags_str, existing_subcategories_str = retrieve_tags()
    user_history = "\n".join(user_context) if user_context else "None"

    api_request_json = {
        "model": "llama3.1-70b",
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are an AI specializing in location classification. "
                    f"Existing subcategories: {existing_subcategories_str}. "
                    f"Existing descriptive tags: {existing_tags_str}. "
                    f"User conversation history: {user_history}. "
                    "Analyze the user's prompt to determine which subcategories (only subcategory names, not categories) it fits into and which descriptive tags apply. "
                    "Return the matching subcategories and descriptive tags. "
                    "If the prompt does not exactly match any existing tag, generate new ones that better capture its essence. "
                    "Return both subcategories and descriptive tags."
                    "If multiple valid subcategories exist and the intent is unclear, return a clarification question."
                )
            },
            {
                "role": "user",
                "content": f"Analyze this prompt: '{prompt}'"
            }
        ],
        "functions": [
            {
                "name": "extract_location_info",
                "description": (
                    "Extract the most relevant subcategories and descriptive tags from the user's prompt based on the provided context. "
                    "For subcategories, compare the prompt with the existing list and return the relevant matches. "
                    "For descriptive tags, do the same by returning matched tags or generating new descriptive words that capture the location's nuances. "
                    "Ensure both subcategories and tags are unique, non-redundant, and appropriately capture the nuances of the location described in the prompt."
                    "If multiple subcategories are found and the intent is unclear, generate a clarification question."
                ),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subcategories": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of top 3 matching subcategories."
                        },
                        "tags": {
                            "type": "object",
                            "properties": {
                                "existed": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "Top 3 descriptive tags that match existing ones."
                                },
                                "new": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "New descriptive tags generated from the prompt."
                                }
                            },
                            "required": ["existed", "new"]
                        },
                        "clarification": {
                            "type": "object",
                            "properties": {
                                "needed": {
                                    "type": "boolean",
                                    "description": "True if clarification is needed, False if the classification is certain."
                                },
                                "question": {
                                    "type": "string",
                                    "description": "Clarification question to ask the user if needed."
                                }
                            },
                            "required": ["needed", "question"]
                        }
                    },
                    "required": ["subcategories", "tags", "clarification"]
                }
            }
        ],
        "function_call": "extract_location_info",
        "max_tokens": 5000,
        "temperature": 0.2,
    }

    response = LLAMA_API.run(api_request_json)
    parsed_json = extract_json(response)

    if parsed_json:
        # Get clarification if needed
        clarification_question = parsed_json['clarification'][
            'question'] if parsed_json['clarification']['needed'] else None

        # Categories and tags
        categories = parsed_json['subcategories']
        tags = parsed_json['tags']['existed']

        return {
            "clarification": clarification_question,
            "categories": categories,
            "tags": tags
        }

    return {"error": "Failed to extract JSON"}


get_location_advice.py:


@timing_decorator
def extract_json(response):
    response_data = response.json()
    print("response:", response_data)

    content = response_data['choices'][0]['message']['content']

    try:
        result = json.loads(content)
        print("✅ Direct JSON parsing successful.")
        return result
    except json.JSONDecodeError:
        print("❌ Direct JSON parsing failed. Trying regex extraction...")

    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        json_str = match.group(0).strip()
        try:
            result = json.loads(json_str)
            print("✅ Regex JSON extraction successful.")
            return result
        except json.JSONDecodeError:
            print("❌ Regex JSON extraction also failed.")

    print("❌ No valid JSON found.")
    return None


@timing_decorator
def format_top_candidates(top_candidates):
    lines = []

    # Expecting top_candidates as a dictionary with keys like "drive", "walk", etc.
    for mode, candidates in top_candidates.items():
        lines.append(f"{mode.capitalize()} Mode:")

        if candidates and len(candidates) > 0:
            for poi in candidates:
                details = [f"Mode: {mode.capitalize()}"]

                # Iterate over all fields in the POI and append them if they have valid values
                for key, value in poi.items():
                    # Exclude None or NaN values (handling both native floats and numpy floats)
                    if value is None or (isinstance(value, (float, np.floating)) and np.isnan(value)):
                        continue

                    # Handle nested dictionaries if any (for example, coordinates)
                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if sub_value is None or (isinstance(sub_value, (float, np.floating)) and np.isnan(sub_value)):
                                continue
                            details.append(
                                f"{sub_key.capitalize()}: {sub_value}")
                    else:
                        details.append(f"{key.capitalize()}: {value}")

                lines.append("\n".join(details))
        else:
            lines.append(
                f"No locations found within the specified route distance for {mode} mode.")

    # If no lines were added, add a default message
    if len(lines) == 0:
        lines.append("No location data available.")

    return "\n\n".join(lines)


@timing_decorator
def get_location_advice(prompt, history, top_candidates, latitude, longitude, search_radius):
    print("Debug: top_candidates before processing:", top_candidates)

    # Ensure that top_candidates is in dictionary format.
    # Make a deep copy to avoid modifying the original data
    if isinstance(top_candidates, list):
        formatted_candidates = {"default": top_candidates}
    else:
        # Create a new dictionary to avoid reference issues
        formatted_candidates = {}
        for mode, candidates in top_candidates.items():
            formatted_candidates[mode] = candidates.copy() if candidates else [
            ]

    # Now format the candidates and keep the formatted text
    context_text = format_top_candidates(formatted_candidates)

    user_history = "\n".join(history) if history else "None"

    api_request_json = {
        "model": "llama3.1-70b",
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are a friendly and helpful assistant who specializes in location recommendations. "
                    "Think of yourself as a knowledgeable local friend who's helping someone navigate the area. "
                    "Make recommendations based on the provided context data about nearby locations.\n\n"
                    "**User Information:**\n"
                    f"- Latitude: {latitude}\n"
                    f"- Longitude: {longitude}\n"
                    f"- Search Radius: {search_radius}\n\n"
                    "**Guidelines:**\n"
                    "- Be conversational and casual, like you're texting a friend.\n"
                    "- If the context contains location data, provide specific recommendations.\n"
                    "- If the context is empty or limited, acknowledge this but still be helpful by:\n"
                    "  • Asking for more details about what they're looking for.\n"
                    "  • Suggesting they increase their search radius.\n"
                    "  • Offering general advice based on what you do know.\n"
                    "- For each recommendation, include key details when available: name, address, distance, and coordinates.\n"
                    "- Keep responses concise but informative.\n"
                    "- Consider transportation modes (walking, driving) in your suggestions.\n"
                    "- Match your tone to the user's query - be upbeat for entertainment queries, practical for necessities.\n\n"
                    "**User Conversation History:**\n"
                    f"{user_history}\n\n"
                    "**Context Information:**\n\n"
                    f"{context_text}\n"
                )
            },
            {
                "role": "user",
                "content": f"Analyze this prompt: '{prompt}'"
            }
        ],
        "functions": [
            {
                "name": "analyze_location_request",
                "description": (
                    "Determines if the prompt is a continuation of the conversation or a new request. "
                    "If it's a continuation, return continuation: true. "
                    "If it's not, return continuation: false and generate a response based on all provided context."
                ),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "continuation": {
                            "type": "boolean",
                            "description": "True if the query is a continuation, False if it's a new request."
                        },
                        "response": {
                            "type": "string",
                            "description": "A response to continue the conversation if continuation is true. A detailed response using context if continuation is false and the context is not empty."
                        }
                    },
                    "required": ["continuation", "response"]
                }
            }
        ],
        "function_call": "analyze_location_request",
        "max_tokens": 7000,
        "temperature": 0.7,
        "top_p": 0.95,
        "frequency_penalty": 0.5,
        "presence_penalty": 0.2,
        "stream": False
    }

    response = LLAMA_API.run(api_request_json)
    parsed_json = extract_json(response)

    if parsed_json:
        return {
            "continuation": parsed_json.get("continuation", False),
            "response": parsed_json.get("response", "I couldn't process your request properly.")
        }

    return {"error": "Failed to process response"}


remove the histroy saving from main.py and then we will add it in the other 2 files and they will update and add the same history file because that is the conversation in full. we will track the full context history and also count tokens on each request. each request will be stored as a single json file.