{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataframe saved to datatest\\cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory where your JSON folders are located\n",
    "base_path = \"NEW_DATA\"\n",
    "folders = [\"nodes\", \"relations\", \"ways\"]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each folder and file, load JSON, and flatten using pd.json_normalize\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            df = pd.json_normalize(data)\n",
    "            # Track source folder and file if needed\n",
    "            df[\"source_folder\"] = folder\n",
    "            df[\"source_file\"] = file\n",
    "            dataframes.append(df)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "# Check if the required columns exist.\n",
    "required_columns = {\"name\", \"coordinates.latitude\", \"coordinates.longitude\"}\n",
    "if not required_columns.issubset(set(combined_df.columns)):\n",
    "    print(\"Required columns are missing from the data. Please ensure your JSON files have 'name', 'coordinates.latitude', and 'coordinates.longitude'.\")\n",
    "else:\n",
    "    # Remove duplicates by keeping only the first occurrence of each unique combination\n",
    "    cleaned_df = combined_df.drop_duplicates(\n",
    "        subset=[\"name\", \"coordinates.latitude\", \"coordinates.longitude\"],\n",
    "        keep=\"first\"\n",
    "    )\n",
    "\n",
    "    # Create output folder \"datatest\" if it doesn't exist.\n",
    "    output_dir = \"datatest\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the cleaned DataFrame to a CSV file.\n",
    "    output_file = os.path.join(output_dir, \"cleaned_data.csv\")\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Cleaned dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: datatest\\cleaned_data1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_file = os.path.join(\"datatest\", \"cleaned_data.csv\")\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out the rows where 'tags' is NaN or unusual AND 'name' contains 'Unknown Place'\n",
    "cleaned_df = df[\n",
    "    ~(\n",
    "        df['tags'].apply(lambda x: x in [\"\", \"[]\", None] or pd.isna(x)) &\n",
    "        df['name'].str.contains(\"Unknown Place\", case=False, na=False)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the file path\n",
    "data_file = os.path.join(\"datatest\", \"cleaned_data1.csv\")\n",
    "\n",
    "# Save the cleaned DataFrame to the file\n",
    "cleaned_df.to_csv(data_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: datatest\\cleaned_data1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast  # to safely evaluate string representations of lists\n",
    "\n",
    "# Define the path for the CSV file\n",
    "data_file = os.path.join(\"datatest\", \"cleaned_data.csv\")\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv(data_file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filter out rows where 'tags' is NaN/unusual AND 'name' contains \"Unknown Place\"\n",
    "cleaned_df = df[\n",
    "    ~(\n",
    "        df['tags'].apply(lambda x: x in [\"\", \"[]\", None] or pd.isna(x)) &\n",
    "        df['name'].str.contains(\"Unknown Place\", case=False, na=False)\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "def clean_tags(tags):\n",
    "    \"\"\"\n",
    "    Convert the tags from string to list (if needed) and remove unwanted entries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert string representation of list to an actual list\n",
    "        tags_list = ast.literal_eval(tags)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If conversion fails, return the original tags\n",
    "        return tags\n",
    "\n",
    "    # Only proceed if the result is a list\n",
    "    if isinstance(tags_list, list):\n",
    "        # Remove 'yes' and '*' from the list\n",
    "        cleaned = [tag for tag in tags_list if tag not in [\"yes\", \"*\"]]\n",
    "        # Optionally, convert the list back to a string for CSV storage\n",
    "        return str(cleaned)\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Use .loc to avoid the SettingWithCopyWarning\n",
    "cleaned_df.loc[:, 'tags'] = cleaned_df['tags'].apply(clean_tags)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(\"datatest\", \"cleaned_data1.csv\")\n",
    "\n",
    "# Save the cleaned DataFrame to the file\n",
    "cleaned_df.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weird values saved to: datatest\\split\\weird_values.csv\n",
      "Clean values saved to: datatest\\split\\clean_values.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "data_file = os.path.join(\"datatest\", \"cleaned_data1.csv\")\n",
    "data = pd.read_csv(data_file)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check for empty strings or unusual entries in 'tags' column\n",
    "weird_values = df[\n",
    "    df['tags'].apply(lambda x: x in [\"\", \"[]\", None] or pd.isna(x))\n",
    "]\n",
    "\n",
    "# Get the remaining rows\n",
    "clean_values = df.drop(weird_values.index)\n",
    "\n",
    "# Create the split folder if it doesn't exist\n",
    "split_folder = os.path.join(\"datatest\", \"split\")\n",
    "os.makedirs(split_folder, exist_ok=True)\n",
    "\n",
    "# Save both DataFrames to separate files\n",
    "weird_file = os.path.join(split_folder, \"weird_values.csv\")\n",
    "clean_file = os.path.join(split_folder, \"clean_values.csv\")\n",
    "\n",
    "weird_values.to_csv(weird_file, index=False)\n",
    "clean_values.to_csv(clean_file, index=False)\n",
    "\n",
    "print(f\"Weird values saved to: {weird_file}\")\n",
    "print(f\"Clean values saved to: {clean_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import folium\n",
    "import ast\n",
    "\n",
    "\n",
    "def tag_contains(tag, target_tag):\n",
    "    \"\"\"\n",
    "    Check if the tag (or list of tags) contains the target tag.\n",
    "    The tag can be a list already or a string representing a list.\n",
    "    \"\"\"\n",
    "    if isinstance(tag, list):\n",
    "        return any(t.lower() == target_tag.lower() for t in tag)\n",
    "\n",
    "    try:\n",
    "        tag_list = ast.literal_eval(tag)\n",
    "        if isinstance(tag_list, list):\n",
    "            return any(t.lower() == target_tag.lower() for t in tag_list)\n",
    "    except Exception:\n",
    "        return target_tag.lower() in tag.lower()\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# Define the path to the CSV file\n",
    "data_file = os.path.join(\"datatest\", \"split\", \"clean_values.csv\")\n",
    "\n",
    "# Load the CSV data\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file {data_file}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Check if 'tags' column exists\n",
    "if 'tags' in df.columns:\n",
    "    # Convert the tags from string to list and flatten them\n",
    "    all_tags = df['tags'].apply(lambda x: ast.literal_eval(\n",
    "        x) if pd.notna(x) else []).explode()\n",
    "\n",
    "    # Get unique tags\n",
    "    unique_tags = all_tags.unique()\n",
    "    print(f\"Unique tags found: {unique_tags}\")\n",
    "\n",
    "    # Create output directory for maps\n",
    "    output_dir = os.path.join(\"datatest\", \"maps\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Loop through each unique tag and create a map\n",
    "    for tag in unique_tags:\n",
    "        # Filter records containing the current tag\n",
    "        tag_df = df[df[\"tags\"].apply(lambda x: tag_contains(x, tag))]\n",
    "\n",
    "        if tag_df.empty:\n",
    "            print(f\"No records found for tag '{tag}'. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of the coordinates for map centering\n",
    "        center_lat = tag_df[\"coordinates.latitude\"].mean()\n",
    "        center_lon = tag_df[\"coordinates.longitude\"].mean()\n",
    "\n",
    "        # Create an interactive folium map centered on the computed location\n",
    "        m = folium.Map(location=[center_lat, center_lon], zoom_start=2)\n",
    "\n",
    "        # Add markers for each record with the current tag\n",
    "        for idx, row in tag_df.iterrows():\n",
    "            lat = row[\"coordinates.latitude\"]\n",
    "            lon = row[\"coordinates.longitude\"]\n",
    "            name = row.get(\"name\", \"Unknown\")\n",
    "            description = row.get(\"description\", \"No description available.\")\n",
    "            location_text = row.get(\"location\", \"No location specified.\")\n",
    "\n",
    "            # Create a popup message with relevant details\n",
    "            popup_text = (\n",
    "                f\"<b>Name:</b> {name}<br>\"\n",
    "                f\"<b>Description:</b> {description}<br>\"\n",
    "                f\"<b>Location:</b> {location_text}\"\n",
    "            )\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                popup=folium.Popup(popup_text, parse_html=True)\n",
    "            ).add_to(m)\n",
    "\n",
    "        # Save each map to the output directory with the tag as the filename\n",
    "        output_map_file = os.path.join(output_dir, f\"{tag}_map.html\")\n",
    "        m.save(output_map_file)\n",
    "        print(f\"Map for tag '{tag}' saved to {output_map_file}.\")\n",
    "\n",
    "else:\n",
    "    print(\"'tags' column not found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tags saved to datatest\\filtered\\filtered_tags.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# List of tags to keep\n",
    "travel_tags = [\n",
    "    \"viewpoint\",\n",
    "    \"attraction\",\n",
    "    \"castle\",\n",
    "    \"fort\",\n",
    "    \"tomb\",\n",
    "    \"ruins\",\n",
    "    \"archaeological_site\",\n",
    "    \"monument\",\n",
    "    \"memorial\",\n",
    "    \"artwork\",\n",
    "    \"museum\",\n",
    "    \"gallery\",\n",
    "    \"park\",\n",
    "    \"garden\",\n",
    "    \"nature_reserve\",\n",
    "    \"playground\",\n",
    "    \"theme_park\",\n",
    "    \"water_park\",\n",
    "    \"miniature_golf\",\n",
    "    \"swimming_pool\",\n",
    "    \"sauna\",\n",
    "    \"zoo\",\n",
    "    \"aquarium\",\n",
    "    \"cinema\",\n",
    "    \"theatre\",\n",
    "    \"cafe\",\n",
    "    \"bar\",\n",
    "    \"pub\",\n",
    "    \"ice_cream\",\n",
    "    \"restaurant\",\n",
    "    \"fast_food\",\n",
    "    \"hotel\",\n",
    "    \"hostel\",\n",
    "    \"guest_house\",\n",
    "    \"chalet\",\n",
    "    \"camp_site\",\n",
    "    \"picnic_site\",\n",
    "    \"fountain\",\n",
    "    \"city_gate\",\n",
    "    \"tower\",\n",
    "    \"clock\",\n",
    "    \"citywalls\",\n",
    "    \"marina\",\n",
    "    \"ferry_terminal\",\n",
    "    \"ship\",\n",
    "    \"aqueduct\",\n",
    "    \"sports_centre\",\n",
    "    \"stadium\",\n",
    "    \"fitness_centre\",\n",
    "    \"golf_course\",\n",
    "    \"track\",\n",
    "    \"pitch\"\n",
    "]\n",
    "\n",
    "# Define the path to the input CSV file\n",
    "input_file = os.path.join(\"datatest\", \"split\", \"clean_values.csv\")\n",
    "\n",
    "# Define the path to the output CSV file\n",
    "output_file = os.path.join(\"datatest\", \"filtered\", \"filtered_tags.csv\")\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Load the CSV data\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file {input_file}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Check if 'tags' column exists\n",
    "if 'tags' in df.columns:\n",
    "    # Function to filter tags\n",
    "    def filter_tags(tag_str):\n",
    "        try:\n",
    "            # Convert the tag string to a list\n",
    "            tags = ast.literal_eval(tag_str)\n",
    "            if isinstance(tags, list):\n",
    "                # Keep only tags that are in the travel_tags list\n",
    "                filtered = [tag for tag in tags if tag in travel_tags]\n",
    "                return filtered if filtered else None  # Return None if list is empty\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    # Apply the filtering to the tags column\n",
    "    df['tags'] = df['tags'].apply(filter_tags)\n",
    "\n",
    "    # Drop rows where tags are now empty\n",
    "    filtered_df = df.dropna(subset=['tags'])\n",
    "\n",
    "    # Save the filtered DataFrame to a new CSV file\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "    print(f\"Filtered tags saved to {output_file}.\")\n",
    "\n",
    "else:\n",
    "    print(\"'tags' column not found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['restaurant' 'fast_food' 'fountain' 'cafe' 'cinema' 'theatre' 'bar'\n",
      " 'ice_cream' 'hotel' 'pub' 'memorial' 'attraction' 'city_gate' 'monument'\n",
      " 'ruins' 'archaeological_site' 'viewpoint' 'castle' 'tomb' 'fort'\n",
      " 'artwork' 'tower' 'clock' 'museum' 'sports_centre' 'playground' 'park'\n",
      " 'swimming_pool' 'fitness_centre' 'stadium' 'garden' 'miniature_golf'\n",
      " 'sauna' 'hostel' 'gallery' 'guest_house' 'picnic_site' 'camp_site'\n",
      " 'aquarium' 'chalet' 'theme_park' 'zoo' 'ship' 'citywalls'\n",
      " 'nature_reserve' 'pitch' 'water_park' 'ferry_terminal' 'track' 'marina'\n",
      " 'golf_course' 'aqueduct']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "data_file = os.path.join(\"datatest\", \"filtered\", \"filtered_tags.csv\")\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"File not found: {data_file}\")\n",
    "else:\n",
    "    data = pd.read_csv(data_file)\n",
    "    df = pd.DataFrame(data)\n",
    "    if 'tags' in df.columns:\n",
    "        # Convert the tags from string to list and flatten them\n",
    "        all_tags = df['tags'].apply(lambda x: ast.literal_eval(x)).explode()\n",
    "        # Get unique tags\n",
    "        unique_tags = all_tags.unique()\n",
    "        print(unique_tags)\n",
    "    else:\n",
    "        print(\"'tags' column not found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26814 entries, 0 to 26813\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   name                   26814 non-null  object \n",
      " 1   description            26814 non-null  object \n",
      " 2   location               26814 non-null  object \n",
      " 3   tags                   26814 non-null  object \n",
      " 4   coordinates.latitude   26814 non-null  float64\n",
      " 5   coordinates.longitude  26814 non-null  float64\n",
      " 6   source_folder          26814 non-null  object \n",
      " 7   source_file            26814 non-null  object \n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(\"datatest\", \"filtered\", \"filtered_tags.csv\")\n",
    "data = pd.read_csv(data_file)\n",
    "df = pd.DataFrame(data)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Geospatial Utility Functions\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def compute_bounding_box(lat, lon, radius_m):\n",
    "    \"\"\"\n",
    "    Compute an approximate bounding box around a point (lat, lon) with a given radius (in meters).\n",
    "    Returns (min_lat, max_lat, min_lon, max_lon).\n",
    "    \"\"\"\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    lat_rad = math.radians(lat)\n",
    "\n",
    "    # Calculate degree offsets\n",
    "    delta_lat = (radius_m / R) * (180 / math.pi)\n",
    "    delta_lon = (radius_m / (R * math.cos(lat_rad))) * (180 / math.pi)\n",
    "\n",
    "    min_lat = lat - delta_lat\n",
    "    max_lat = lat + delta_lat\n",
    "    min_lon = lon - delta_lon\n",
    "    max_lon = lon + delta_lon\n",
    "    return min_lat, max_lat, min_lon, max_lon\n",
    "\n",
    "\n",
    "def filter_by_bounding_box_and_tag(df, user_lat, user_lon, radius_m, search_tag):\n",
    "    \"\"\"\n",
    "    Quickly filter POIs that fall within a bounding box around the user's location\n",
    "    and contain the specified tag.\n",
    "    \"\"\"\n",
    "    min_lat, max_lat, min_lon, max_lon = compute_bounding_box(\n",
    "        user_lat, user_lon, radius_m)\n",
    "\n",
    "    # Filter by bounding box\n",
    "    filtered_df = df[\n",
    "        (df['coordinates.latitude'] >= min_lat) &\n",
    "        (df['coordinates.latitude'] <= max_lat) &\n",
    "        (df['coordinates.longitude'] >= min_lon) &\n",
    "        (df['coordinates.longitude'] <= max_lon)\n",
    "    ]\n",
    "\n",
    "    # Filter by search tag (case insensitive)\n",
    "    filtered_df = filtered_df[filtered_df['tags'].str.contains(\n",
    "        search_tag, case=False, na=False)]\n",
    "\n",
    "    # Convert filtered DataFrame to a list of dictionaries for easier processing later\n",
    "    candidates = filtered_df.to_dict(orient='records')\n",
    "    return candidates\n",
    "\n",
    "# ---------------------------\n",
    "# Routing Functions using OSMnx & NetworkX (Car Mode)\n",
    "# ---------------------------\n",
    "\n",
    "\n",
    "def get_network_graph(user_lat, user_lon, radius_m, travel_mode='drive'):\n",
    "    \"\"\"\n",
    "    Download a street network graph centered on the user's location.\n",
    "    Supports multiple travel modes like 'drive' and 'walk'.\n",
    "    \"\"\"\n",
    "    graph_dist = radius_m * 2\n",
    "    try:\n",
    "        graph = ox.graph_from_point(\n",
    "            (user_lat, user_lon), dist=graph_dist, network_type=travel_mode)\n",
    "        return graph\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving network graph for {travel_mode}:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_route_distance(graph, user_lat, user_lon, candidate_lat, candidate_lon):\n",
    "    \"\"\"\n",
    "    Compute the route (network) distance between the user's location and the candidate's location.\n",
    "    Returns distance in meters.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_node = ox.distance.nearest_nodes(graph, user_lon, user_lat)\n",
    "        candidate_node = ox.distance.nearest_nodes(\n",
    "            graph, candidate_lon, candidate_lat)\n",
    "        route_length = nx.shortest_path_length(\n",
    "            graph, user_node, candidate_node, weight='length')\n",
    "        return route_length\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error computing route for candidate at ({candidate_lat}, {candidate_lon}):\", e)\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def get_top_n_by_route_distance_for_all_modes(candidates, user_lat, user_lon, radius_m, n=5):\n",
    "    \"\"\"\n",
    "    Compute route distances for all candidates using both driving and walking modes.\n",
    "    \"\"\"\n",
    "    modes = ['drive', 'walk']\n",
    "    all_results = {}\n",
    "\n",
    "    for mode in modes:\n",
    "        graph = get_network_graph(\n",
    "            user_lat, user_lon, radius_m, travel_mode=mode)\n",
    "        if graph is None:\n",
    "            print(\n",
    "                f\"Failed to retrieve the network graph for {mode}. Skipping this mode.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate route distance for each candidate\n",
    "        for poi in candidates:\n",
    "            candidate_lat = poi[\"coordinates.latitude\"]\n",
    "            candidate_lon = poi[\"coordinates.longitude\"]\n",
    "            route_distance = get_route_distance(\n",
    "                graph, user_lat, user_lon, candidate_lat, candidate_lon)\n",
    "            poi[f\"{mode}_route_distance_m\"] = route_distance\n",
    "\n",
    "        # Filter to only those POIs that are within the route distance threshold\n",
    "        candidates_within_radius = [\n",
    "            poi for poi in candidates if poi[f\"{mode}_route_distance_m\"] <= radius_m]\n",
    "\n",
    "        # Sort by route distance (shortest first)\n",
    "        candidates_within_radius.sort(\n",
    "            key=lambda x: x[f\"{mode}_route_distance_m\"])\n",
    "\n",
    "        all_results[mode] = candidates_within_radius[:n]\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution: Updated for Both Driving and Walking\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the POI data from the CSV file.\n",
    "    data_file = os.path.join(\"datatest\", \"filtered\", \"filtered_tags.csv\")\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(\"Data Information:\")\n",
    "    df.info()\n",
    "\n",
    "    # Simulated user query parameters:\n",
    "    user_lat = 40.985660   # Example: Istanbul city center latitude\n",
    "    user_lon = 29.027361   # Example: Istanbul city center longitude\n",
    "    radius_m = 1000        # 1 km search radius\n",
    "    search_tag = \"cinema\"    # Example tag to filter for\n",
    "\n",
    "    # --- Step 1: Candidate Filtering by Bounding Box and Tag ---\n",
    "    candidates = filter_by_bounding_box_and_tag(\n",
    "        df, user_lat, user_lon, radius_m, search_tag)\n",
    "    print(\"Candidates after bounding box and tag filtering:\")\n",
    "    for poi in candidates:\n",
    "        print(\n",
    "            f\"  {poi['name']} at ({poi['coordinates.latitude']}, {poi['coordinates.longitude']}) with tags: {poi['tags']}\")\n",
    "\n",
    "    # --- Step 2: Geospatial Analysis via Route Distances (Drive & Walk) ---\n",
    "    top_candidates = get_top_n_by_route_distance_for_all_modes(\n",
    "        candidates, user_lat, user_lon, radius_m, n=5)\n",
    "\n",
    "    # Display results for both modes\n",
    "    print(\"\\nTop candidates based on route distances:\")\n",
    "    for mode, results in top_candidates.items():\n",
    "        print(f\"\\n--- {mode.capitalize()} Mode ---\")\n",
    "        if results:\n",
    "            for poi in results:\n",
    "                print(\n",
    "                    f\"{poi['name']} - Route Distance: {poi[f'{mode}_route_distance_m']:.2f} meters\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"No locations found within the specified route distance for {mode} mode.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
