history_manager.py:

import json
import os
import time
import uuid
from typing import Dict, List, Any, Optional


class HistoryManager:
    """
    Centralized history manager to store all conversation data in a single file.
    """

    def __init__(self, history_dir="chat_history"):
        self.history_dir = history_dir
        os.makedirs(self.history_dir, exist_ok=True)

    def get_history_file_path(self, conversation_id: str) -> str:
        """
        Get the path to the history file for a specific conversation.
        """
        return os.path.join(self.history_dir, f"{conversation_id}_history.json")

    def create_conversation(self) -> str:
        """
        Create a new conversation and return its ID.
        """
        conversation_id = str(uuid.uuid4())
        history_file = self.get_history_file_path(conversation_id)

        # Initialize history file with empty structure
        initial_data = {
            "conversation_id": conversation_id,
            "created_at": int(time.time()),
            "messages": []
        }

        with open(history_file, 'w') as f:
            json.dump(initial_data, f, indent=2)

        return conversation_id

    def get_conversation(self, conversation_id: str) -> Dict:
        """
        Get the full conversation history.
        """
        history_file = self.get_history_file_path(conversation_id)

        if not os.path.exists(history_file):
            # Create new conversation if doesn't exist
            return {
                "conversation_id": conversation_id,
                "created_at": int(time.time()),
                "messages": []
            }

        try:
            with open(history_file, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError:
            # Handle corrupted files
            return {
                "conversation_id": conversation_id,
                "created_at": int(time.time()),
                "messages": []
            }

    def get_messages(self, conversation_id: str) -> List[Dict]:
        """
        Get just the messages part of the conversation history.
        """
        conversation = self.get_conversation(conversation_id)
        return conversation.get("messages", [])

    def get_formatted_history(self, conversation_id: str) -> List[str]:
        """
        Return history in a format suitable for context window.
        """
        messages = self.get_messages(conversation_id)
        formatted = []

        for msg in messages:
            if msg.get("role") == "user":
                formatted.append(f"User: {msg.get('content', '')}")
            elif msg.get("role") == "assistant":
                formatted.append(f"Assistant: {msg.get('content', '')}")

        return formatted

    def add_message(self, conversation_id: str, role: str, content: str,
                    metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Add a message to the conversation history.
        """
        history_file = self.get_history_file_path(conversation_id)
        conversation = self.get_conversation(conversation_id)

        message = {
            "role": role,
            "content": content,
            "timestamp": int(time.time())
        }

        if metadata:
            message["metadata"] = metadata

        conversation["messages"].append(message)

        with open(history_file, 'w') as f:
            json.dump(conversation, f, indent=2)

    def add_user_message(self, conversation_id: str, content: str,
                         metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Add a user message to history.
        """
        self.add_message(conversation_id, "user", content, metadata)

    def add_assistant_message(self, conversation_id: str, content: str,
                              metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        Add an assistant message to history.
        """
        self.add_message(conversation_id, "assistant", content, metadata)

    def add_llm_interaction(self, conversation_id: str,
                            prompt: str,
                            response: Dict,
                            request_data: Dict,
                            top_candidates: Optional[Dict] = None) -> None:
        """
        Add a full LLM interaction (both request and response) to history.
        """
        # Add user message first
        self.add_user_message(conversation_id, prompt)

        # Add assistant response with full metadata
        response_text = ""
        if isinstance(response, dict):
            response_text = response.get("response", str(response))
        else:
            response_text = str(response)

        metadata = {
            "full_request": request_data,
            "full_response": response,
            "timestamp": int(time.time())
        }

        if top_candidates:
            metadata["top_candidates"] = top_candidates

        self.add_assistant_message(conversation_id, response_text, metadata)

    def get_top_candidates(self, conversation_id: str) -> Dict:
        """
        Get the most recent top_candidates from the conversation history.
        """
        messages = self.get_messages(conversation_id)

        # Search backwards to find the most recent top_candidates
        for msg in reversed(messages):
            if msg.get("role") == "assistant" and msg.get("metadata"):
                if "top_candidates" in msg["metadata"]:
                    return msg["metadata"]["top_candidates"]

        return {}


llamarequest.py:
from config import LLAMA_API
import numpy as np
from config import TAGS_LIST, LLAMA_API, CATEGORY_SUBCATEGORY_LIST
import re
import json
import pandas as pd
import os
import uuid
import time
from src.utils import timing_decorator
from src.history_manager import HistoryManager


def count_tokens(text):
    """
    Approximate token counter - actual implementation would depend on the tokenizer
    This is a simple approximation based on whitespace and punctuation
    """
    # Split by whitespace
    tokens = text.split()
    # Account for punctuation
    token_count = len(tokens)
    # Add estimated tokens for punctuation and special characters
    punctuation_count = sum(1 for char in text if char in '.,;:!?()[]{}"\'')
    return token_count + punctuation_count


@timing_decorator
def retrieve_tags():
    """
    Retrieves tags and subcategories from CSV files and returns them as formatted strings.
    """
    tags_string = "None"
    subcategory_string = "None"

    # Retrieve tags
    if os.path.exists(TAGS_LIST):
        tags_df = pd.read_csv(TAGS_LIST)
        if 'tags' in tags_df.columns:
            tags_list = tags_df['tags'].dropna().tolist()
            tags_string = ", ".join(tags_list) if tags_list else "None"

    # Retrieve subcategories
    if os.path.exists(CATEGORY_SUBCATEGORY_LIST):
        subcategory_df = pd.read_csv(CATEGORY_SUBCATEGORY_LIST)
        if 'category' in subcategory_df.columns and 'subcategory' in subcategory_df.columns:
            # Group by 'category' and aggregate the subcategories into a comma-separated string
            grouped = subcategory_df.groupby('category')['subcategory'].apply(
                lambda x: ",".join(x)).reset_index()

            # Format the string as Category: subcat,subcat,...
            subcategory_string = "\n".join(
                [f"{row['category']}: {row['subcategory']}" for _, row in grouped.iterrows()])
            subcategory_string = subcategory_string if subcategory_string else "None"

    return tags_string, subcategory_string


@timing_decorator
def extract_json(response):
    response_data = response.json()
    print("response:", response_data)

    content = response_data['choices'][0]['message']['content']

    try:
        result = json.loads(content)
        print("✅ Direct JSON parsing successful.")
        return result
    except json.JSONDecodeError:
        print("❌ Direct JSON parsing failed. Trying regex extraction...")

    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        json_str = match.group(0).strip()
        try:
            result = json.loads(json_str)
            print("✅ Regex JSON extraction successful.")
            return result
        except json.JSONDecodeError:
            print("❌ Regex JSON extraction also failed.")

    print("❌ No valid JSON found.")
    return None


def save_request_data(conversation_id, request_type, prompt, context, system_content, response, token_counts):
    """
    Save request data to a JSON file
    """
    # Create a directory for requests if it doesn't exist
    os.makedirs("requests", exist_ok=True)

    # Create a unique filename
    timestamp = int(time.time())
    filename = f"requests/{conversation_id}_{request_type}_{timestamp}.json"

    data = {
        "conversation_id": conversation_id,
        "request_type": request_type,
        "timestamp": timestamp,
        "prompt": prompt,
        "context": context,
        "system_content": system_content,
        "response": response,
        "token_counts": token_counts
    }

    try:
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        print(f"Request saved to {filename}")
    except Exception as e:
        print(f"Error saving request data: {e}")


@timing_decorator
def llm_api(prompt, user_context=None, conversation_id=None, history_manager=None):
    """
    LLM function that extracts subcategories and descriptive tags.
    If multiple subcategories are found, the LLM itself will ask for more detail.
    """
    # Generate a temporary conversation ID if none provided
    if conversation_id is None:
        conversation_id = str(uuid.uuid4())

    # Create a history manager if not provided
    if history_manager is None:
        history_manager = HistoryManager()

    existing_tags_str, existing_subcategories_str = retrieve_tags()
    user_history = "\n".join(user_context) if user_context else "None"

    system_content = (
        "You are an AI specializing in location classification. "
        f"Existing subcategories: {existing_subcategories_str}. "
        f"Existing descriptive tags: {existing_tags_str}. "
        f"User conversation history: {user_history}. "
        "Analyze the user's prompt to determine which subcategories (only subcategory names, not categories) it fits into and which descriptive tags apply. "
        "Return the matching subcategories and descriptive tags. "
        "If the prompt does not exactly match any existing tag, generate new ones that better capture its essence. "
        "Return both subcategories and descriptive tags."
        "If multiple valid subcategories exist and the intent is unclear, return a clarification question."
    )

    api_request_json = {
        "model": "llama3.1-70b",
        "messages": [
            {
                "role": "system",
                "content": system_content
            },
            {
                "role": "user",
                "content": f"Analyze this prompt: '{prompt}'"
            }
        ],
        "functions": [
            {
                "name": "extract_location_info",
                "description": (
                    "Extract the most relevant subcategories and descriptive tags from the user's prompt based on the provided context. "
                    "For subcategories, compare the prompt with the existing list and return the relevant matches. "
                    "For descriptive tags, do the same by returning matched tags or generating new descriptive words that capture the location's nuances. "
                    "Ensure both subcategories and tags are unique, non-redundant, and appropriately capture the nuances of the location described in the prompt."
                    "If multiple subcategories are found and the intent is unclear, generate a clarification question."
                ),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subcategories": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of top 3 matching subcategories."
                        },
                        "tags": {
                            "type": "object",
                            "properties": {
                                "existed": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "Top 3 descriptive tags that match existing ones."
                                },
                                "new": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "New descriptive tags generated from the prompt."
                                }
                            },
                            "required": ["existed", "new"]
                        },
                        "clarification": {
                            "type": "object",
                            "properties": {
                                "needed": {
                                    "type": "boolean",
                                    "description": "True if clarification is needed, False if the classification is certain."
                                },
                                "question": {
                                    "type": "string",
                                    "description": "Clarification question to ask the user if needed."
                                }
                            },
                            "required": ["needed", "question"]
                        }
                    },
                    "required": ["subcategories", "tags", "clarification"]
                }
            }
        ],
        "function_call": "extract_location_info",
        "max_tokens": 5000,
        "temperature": 0.2,
    }

    # Count tokens in request
    input_tokens = count_tokens(system_content) + count_tokens(prompt)

    # Make API request
    response = LLAMA_API.run(api_request_json)

    # Extract response content
    response_data = response.json()
    response_content = response_data['choices'][0]['message']['content']

    # Count tokens in response
    output_tokens = count_tokens(response_content)

    # Calculate total tokens
    total_tokens = input_tokens + output_tokens

    # Token counts dictionary
    token_counts = {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": total_tokens
    }

    parsed_json = extract_json(response)

    result = {}
    if parsed_json:
        # Get clarification if needed
        clarification_question = parsed_json['clarification'][
            'question'] if parsed_json['clarification']['needed'] else None

        # Categories and tags
        categories = parsed_json['subcategories']
        tags = parsed_json['tags']['existed']

        result = {
            "clarification": clarification_question,
            "categories": categories,
            "tags": tags
        }
    else:
        result = {"error": "Failed to extract JSON"}

    # Create a full request data dictionary for history
    request_data = {
        "request_type": "llm_classification",
        "timestamp": int(time.time()),
        "prompt": prompt,
        "context": user_context,
        "system_content": system_content,
        "api_request": api_request_json,
        "api_response": response_data,
        "token_counts": token_counts
    }

    # Save to history
    history_manager.add_llm_interaction(
        conversation_id=conversation_id,
        prompt=prompt,
        response=result,
        request_data=request_data
    )

    return result


i want a clean and order and better chat history structure, may we can change the format and so on. how can we make it better so no repeated text appears and system prompt are repeated accross every run and that is redundant, better organize the chat history file and then my llamarequest file