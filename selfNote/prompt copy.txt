this 2 files send api request to the llama api and get a response. they are similar so i want to build a interface and then inside the interface we can write the common functions that will be used in both files. In testing it is kinda expensive to always send a new request to the api. So we can cache the response and use it again. cache it into a file.json or what other pyc or something so when the input is the same we can just read the file and return from there. do you have a better idea? and then in prod we can easily turn off the caching system or even upgrade the caching system to a more better one so we can even build a class for that. give me your thoughts


# src/llamarequest.py
import os
import pandas as pd
from config import LLAMA_API, TAGS_LIST, CATEGORY_SUBCATEGORY_LIST
from src.utils import timing_decorator
from src.data_types import LLMResponse
from typing import List, Optional
from src.function_api_builder import create_classification_request
from src.logger_setup import get_logger
import json


def extract_content(response):
    """Extracts the JSON content from the response's 'content' field."""
    try:
        # Navigate to the content field
        content_str = response.get("choices", [{}])[0].get(
            "message", {}).get("content", "")

        # Parse the JSON
        extracted_json = json.loads(content_str)

        return extracted_json
    except (json.JSONDecodeError, IndexError, KeyError) as e:
        print(f"Error extracting content: {e}")
        return None


@timing_decorator
def llm_api(prompt: str, subcategories) -> LLMResponse:
    logger = get_logger()
    logger.info("Calling LLM API with the provided prompt.")

    # user_history = history if history else "No previous conversation"
    existing_subcategories_str = subcategories
    print(existing_subcategories_str)

    logger.debug(f"Existing subcategories: {existing_subcategories_str}")
    # logger.debug("User history: %s", user_history.replace('\n', ' || '))

    # Prepare the API request
    api_request_json = create_classification_request(
        prompt, existing_subcategories_str, )
    logger.debug(
        f"API request JSON from create_classification_request: {api_request_json}")

    # Call the LLAMA API
    try:
        response = LLAMA_API.run(api_request_json)
        print("Response from LLAMA API:", response.json())
        logger.info("Received response from LLAMA API.")
        logger.debug(f"Response: {response.json()}")
    except Exception as e:
        logger.error(f"Error calling LLAMA API: {e}")
        return LLMResponse({"error": "Failed to call LLAMA API"})

    # Extract and parse JSON from the response
    extracted_json = extract_content(response.json())

    return extracted_json


# src/get_location_advice.py

from typing import List, Optional, Dict
import re
import json
import numpy as np
import uuid
from config import LLAMA_API
from src.utils import timing_decorator

from src.data_types import TopCandidates, LocationAdviceResponse
from src.function_api_builder import build_location_request, build_location_request_search
from src.logger_setup import get_logger


def format_top_candidates(top_candidates: TopCandidates) -> str:
    """
    Format top candidate points of interest into a readable string.
    Handles numpy types and None values properly.
    """
    logger = get_logger()
    lines = []

    for mode, candidates in top_candidates.items():
        lines.append(f"{mode.capitalize()} Mode:")

        if candidates and len(candidates) > 0:
            for poi in candidates:
                details = [f"Mode: {mode.capitalize()}"]

                for key, value in poi.items():
                    # Convert numpy types to Python native types
                    if isinstance(value, np.generic):
                        value = value.item()  # Replaced np.asscalar with .item()

                    if value is None or (isinstance(value, float) and np.isnan(value)):
                        continue

                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if isinstance(sub_value, np.generic):
                                sub_value = sub_value.item()  # Replaced np.asscalar with .item()
                            if sub_value is None or (isinstance(sub_value, float) and np.isnan(sub_value)):
                                continue
                            details.append(
                                f"{sub_key.capitalize()}: {sub_value}")
                    else:
                        details.append(f"{key.capitalize()}: {value}")

                lines.append("\n".join(details))
        else:
            lines.append(
                f"No locations found within the specified route distance for {mode} mode.")

    logger.debug("Formatted top candidates: %s", "\n\n".join(lines))
    return "\n\n".join(lines) if lines else "No location data available."


def extract_content(response):
    """Extracts the JSON content from the response's 'content' field."""
    try:
        # Navigate to the content field
        content_str = response.get("choices", [{}])[0].get(
            "message", {}).get("content", "")

        # Parse the JSON
        extracted_json = json.loads(content_str)

        return extracted_json
    except (json.JSONDecodeError, IndexError, KeyError) as e:
        print(f"Error extracting content: {e}")
        return None


@timing_decorator
def get_location_advice(prompt, history, top_candidates: TopCandidates,
                        latitude, longitude, search_radius, flag = False) -> LocationAdviceResponse:
    """Main function to get location advice with structured response handling

    Returns:
        Dict containing:
        - response: str - The text response to the user
        - continuation: bool - Whether the conversation should continue
        - recommendations: List[Dict[str, str]] - List of recommended locations with details
        - error: Optional[str] - Error message if any
        - token_counts: Dict[str, int] - Token usage statistics
        - conversation_id: str - Unique identifier for the conversation
    """
    logger = get_logger()
    """Main function to get location advice with structured response handling"""

    # Format context and history
    formatted_candidates = format_top_candidates(top_candidates)

    # Handle history - now expecting pre-formatted string
    user_history = history if history else "No previous conversation"

    logger.debug("User history: %s", user_history.replace('\n', ' || '))
    logger.debug("Formatted candidates: %s", formatted_candidates)

    if flag == True:
        # Build API request
        api_request = build_location_request_search(
            prompt, formatted_candidates, user_history,
            latitude, longitude, search_radius
        )
    else:
        # Build API request
        api_request = build_location_request(
            prompt, formatted_candidates, user_history,
            latitude, longitude, search_radius
        )
    logger.debug(
        f"API request JSON from build_location_request: {api_request}")

    try:
        # Execute API call
        response = LLAMA_API.run(api_request)
        logger.info("Received response from LLAMA API.")
        logger.debug(f"Response: {response.json()}")

        # Process response
        extracted_json = extract_content(response.json())

    except Exception as e:
        logger.error("Location Advice API failed: %s", e)

    logger.debug("API response processed with result: %s", extracted_json)

    return extracted_json
