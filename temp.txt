main.py: 

import os
from src.history_manager import HistoryManager
from src.llamarequest import llm_api
from src.poi_filter import get_poi_data
from src.get_top_candidates import find_top_candidates
from src.get_location_advice import get_location_advice
from src.data_types import LLMResponse, TopCandidates


def handle_clarification(llm_response: LLMResponse, prompt: str, formatted_history: str, conversation_id: str, history_manager: HistoryManager) -> str:
    clarification = llm_response.get("clarification")
    if clarification:
        if isinstance(clarification, str):
            question = clarification
        else:
            question = clarification.get('question', '')

        additional_input = input(
            f"Clarification Needed: {question}\nProvide clarification: ")

        # Add the user's clarification as a new message
        history_manager.add_user_message(conversation_id, additional_input)

        # Fetch updated history including the new message
        updated_history = history_manager.get_formatted_history(
            conversation_id)

        # Re-run the llm_api with the new input and updated history
        llm_response = llm_api(
            prompt=additional_input,  # Use the new input as the prompt
            user_context=updated_history,
            conversation_id=conversation_id,
            history_manager=history_manager
        )

        # Check for further clarification recursively
        if llm_response.get("clarification"):
            return handle_clarification(llm_response, additional_input, updated_history, conversation_id, history_manager)

    return prompt


def process_new_query(user_prompt: str, formatted_history: str, conversation_id: str,
                      history_manager: HistoryManager, latitude: float, longitude: float,
                      search_radius: int, num_candidates: int) -> TopCandidates:
    """Process a new user query to get new top candidates."""
    # Add the user message to history before processing
    history_manager.add_user_message(conversation_id, user_prompt)
    # Get initial LLM classification response
    llm_response = llm_api(
        prompt=user_prompt,
        user_context=formatted_history,
        conversation_id=conversation_id,
        history_manager=history_manager
    )

    print(llm_response)
    print(llm_response.get("categories", []))

    if llm_response is None or 'error' in llm_response:
        print("Error in LLM processing. Please try again.")
        return None

    # Handle any clarification request from the LLM
    user_prompt = handle_clarification(
        llm_response, user_prompt, formatted_history, conversation_id, history_manager)

    if not user_prompt:
        return None  # If there's an error, return None

    categories = llm_response.get("categories", [])
    print(f"Categories to search for: {categories}")

    candidates = get_poi_data(
        latitude, longitude, search_radius, categories
    )

    if not candidates:
        print("No POIs found based on your criteria.")
        return None

    top_candidates = find_top_candidates(
        candidates, latitude, longitude, search_radius, num_candidates
    )

    if not isinstance(top_candidates, dict):
        top_candidates = {"default": top_candidates}

    return top_candidates


def main():
    # Initialize shared history manager
    history_manager = HistoryManager()

    conversation_id_input = input(
        "Enter conversation ID (leave blank for new conversation): ")
    if conversation_id_input.strip():
        conversation_id = conversation_id_input.strip()
        if not os.path.exists(history_manager.get_history_file_path(conversation_id)):
            print(f"Creating new conversation with ID: {conversation_id}")
            history_manager.create_conversation(conversation_id)
    else:
        conversation_id = history_manager.create_conversation()
        print(f"Created new conversation with ID: {conversation_id}")

    messages = history_manager.get_messages(conversation_id)
    if messages:
        print("\n--- Conversation History ---")
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                print(f"User: {content}")
            elif role == "assistant":
                print(f"Bot: {content}")
        print("--- End of History ---\n")
    else:
        print("No previous messages in this conversation.")

    # Default location parameters
    latitude = 41.064108
    longitude = 29.031473
    search_radius = 2000
    num_candidates = 2

    top_candidates = None
    reuse_prompt = False
    last_prompt = ""

    while True:
        if not reuse_prompt:
            user_prompt = input(
                "\nEnter your prompt (or type 'exit' to quit): ")
            if user_prompt.lower() == 'exit':
                break
            last_prompt = user_prompt
        else:
            user_prompt = last_prompt
            reuse_prompt = False
            print(f"\nReusing last prompt: {user_prompt}")

        formatted_history = history_manager.get_formatted_history(
            conversation_id)
        print("\nFormatted History:", formatted_history)

        # If we don't have top candidates yet, process a new query
        if not top_candidates:
            top_candidates = process_new_query(
                user_prompt, formatted_history, conversation_id,
                history_manager, latitude, longitude, search_radius, num_candidates
            )
            if not top_candidates:
                continue  # Skip to next loop iteration if processing failed

        # Get location advice based on the top candidates
        try:
            location_advice = get_location_advice(
                prompt=user_prompt,
                history=formatted_history,
                top_candidates=top_candidates,
                latitude=latitude,
                longitude=longitude,
                search_radius=search_radius,
                conversation_id=conversation_id,
                history_manager=history_manager
            )
        except Exception as e:
            print(f"Error during location advice processing: {e}")
            continue

        response_text = location_advice.get(
            "response", "No response received.")
        print("\nLocation Advice:", response_text)

        # Ask for follow-up input regardless of the continuation value
        user_prompt = input(
            "\nEnter follow-up question (or type 'new' to start new search): ")
        if user_prompt.lower() == 'exit':
            break

        # Add the follow-up question to history and refresh formatted_history
        history_manager.add_user_message(conversation_id, user_prompt)
        formatted_history = history_manager.get_formatted_history(
            conversation_id)

        # Get second location advice with the follow-up question
        try:
            location_advice = get_location_advice(
                prompt=user_prompt,
                history=formatted_history,
                top_candidates=top_candidates,
                latitude=latitude,
                longitude=longitude,
                search_radius=search_radius,
                conversation_id=conversation_id,
                history_manager=history_manager
            )
        except Exception as e:
            print(f"Error during location advice processing: {e}")
            continue

        # Check if continuation is a string and handle accordingly
        continuation = str(location_advice.get(
            "continuation", "false")).lower()
        response_text = location_advice.get(
            "response", "No response received.")
        print("\nLocation Advice:", response_text)

        if continuation == "false":
            break

        # Now check if we should enter the continuation loop
        while continuation == "true":
            user_prompt = input(
                "\nEnter your prompt (or type 'exit' to quit): ")
            if user_prompt.lower() == 'exit':
                break
            last_prompt = user_prompt  # Update the last prompt

            # Add the follow-up question to history and refresh formatted_history
            history_manager.add_user_message(conversation_id, user_prompt)
            formatted_history = history_manager.get_formatted_history(
                conversation_id)

            try:
                location_advice = get_location_advice(
                    prompt=user_prompt,
                    history=formatted_history,
                    top_candidates=top_candidates,
                    latitude=latitude,
                    longitude=longitude,
                    search_radius=search_radius,
                    conversation_id=conversation_id,
                    history_manager=history_manager
                )
            except Exception as e:
                print(f"Error during location advice processing: {e}")
                continue

            continuation = str(location_advice.get(
                "continuation", "false")).lower()
            response_text = location_advice.get(
                "response", "No response received.")
            print("\nLocation Advice:", response_text)

            if continuation == "false":
                break

        if continuation == "false":
            # If continuation is false, clear top_candidates to get new ones on next iteration
            print("Starting new recommendation context with the same prompt.")
            top_candidates = None
            reuse_prompt = True  # Flag to reuse the last prompt


if __name__ == "__main__":
    main()


get_location_advice.py:
import json
import os
import time
import uuid
import logging
from typing import Dict, List, Any, Optional
from src.data_types import Message, Conversation, TopCandidates, LLMResponse


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class HistoryManager:
    """
    Centralized history manager to store all conversation data in a single file.
    """

    def __init__(self, history_dir="chat_history"):
        self.history_dir = history_dir
        os.makedirs(self.history_dir, exist_ok=True)

    def get_history_file_path(self, conversation_id: str) -> str:
        return os.path.join(self.history_dir, f"{conversation_id}_history.json")

    def _default_conversation(self, conversation_id: str) -> Conversation:
        return {
            "conversation_id": conversation_id,
            "created_at": int(time.time()),
            "messages": []
        }

    def create_conversation(self, conversation_id: str) -> Conversation:
        if conversation_id is None:
            conversation_id = str(uuid.uuid4())
        history_file = self.get_history_file_path(conversation_id)
        initial_data = self._default_conversation(conversation_id)
        with open(history_file, 'w') as f:
            json.dump(initial_data, f, indent=2)
        return conversation_id

    def get_conversation(self, conversation_id: str) -> Dict:
        history_file = self.get_history_file_path(conversation_id)
        if not os.path.exists(history_file):
            return self._default_conversation(conversation_id)
        try:
            with open(history_file, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.error("JSON decode error in file: %s", history_file)
            return self._default_conversation(conversation_id)

    def get_messages(self, conversation_id: str) -> List[Dict]:
        conversation = self.get_conversation(conversation_id)
        return conversation.get("messages", [])


    def get_formatted_history(self, conversation_id: str) -> str:
        """Returns conversation history in a format suitable for context."""
        messages = self.get_messages(conversation_id)
        if not messages:
            return ""  # Return empty string instead of "No previous conversation"

        formatted = []
        for msg in messages:
            if msg.get("role") == "user":
                formatted.append(f"User: {msg.get('content', '')}")
            elif msg.get("role") == "assistant":
                formatted.append(f"Assistant: {msg.get('content', '')}")

        return "\n".join(formatted)

    def add_message(self, conversation_id: str, role: str, content: str,
                    metadata: Optional[Dict[str, Any]] = None) -> None:
        history_file = self.get_history_file_path(conversation_id)
        conversation = self.get_conversation(conversation_id)
        message = {
            "role": role,
            "content": content,
            "timestamp": int(time.time())
        }
        if metadata:
            message["metadata"] = metadata
        conversation["messages"].append(message)
        with open(history_file, 'w') as f:
            json.dump(conversation, f, indent=2)

    def add_user_message(self, conversation_id: str, content: str,
                         metadata: Optional[Dict[str, Any]] = None) -> None:
        self.add_message(conversation_id, "user", content, metadata)

    def add_assistant_message(self, conversation_id: str, content: str,
                              metadata: Optional[Dict[str, Any]] = None) -> None:
        self.add_message(conversation_id, "assistant", content, metadata)


    def add_llm_interaction(self, conversation_id: str,
                            prompt: str,
                            response: Any,
                            request_data: Dict,
                            top_candidates: Optional[TopCandidates] = None) -> None:
        # Add user prompt to history
        self.add_user_message(conversation_id, prompt)

        # Extract response text consistently
        if isinstance(response, dict):
            response_text = response.get("response", "")
            if not response_text and "error" in response:
                response_text = f"Error: {response['error']}"
        else:
            response_text = str(response)

        # Create consistent metadata
        metadata = {
            "request_type": request_data.get("request_type", "unknown"),
            "timestamp": int(time.time()),
            "tokens": request_data.get("token_counts", {})
        }

        if top_candidates:
            metadata["top_candidates"] = top_candidates

        # Add assistant response with metadata
        self.add_assistant_message(conversation_id, response_text, metadata)


    def get_top_candidates(self, conversation_id: str) -> TopCandidates:
        messages = self.get_messages(conversation_id)
        # Search backwards for the most recent top_candidates
        for msg in reversed(messages):
            if msg.get("role") == "assistant" and msg.get("metadata"):
                if "top_candidates" in msg["metadata"]:
                    return msg["metadata"]["top_candidates"]
        return TopCandidates({})

llamarequest.py:
import os
import time
import uuid
import json
import re
import pandas as pd
import logging
from src.history_manager import HistoryManager
from config import LLAMA_API, TAGS_LIST, CATEGORY_SUBCATEGORY_LIST
from src.utils import count_tokens, timing_decorator
from src.data_types import LLMResponse
from typing import List, Optional

import os
import logging
from datetime import datetime


def setup_logging(script_name: str) -> logging.Logger:
    """
    Set up logging to create a unique log file based on the script name and timestamp.

    Parameters:
    - script_name: Name of the script (used to create the log folder).

    Returns:
    - logger: Configured logger instance.
    """
    # Create the log directory based on the script name
    log_directory = f'logs/{script_name}'
    os.makedirs(log_directory, exist_ok=True)

    # Generate a unique log filename based on the current timestamp
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    log_filename = os.path.join(log_directory, f"log_{timestamp}.log")

    # Set up logging to both the console and the log file
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),  # Log to the generated log file
            logging.StreamHandler()  # Also log to the console
        ]
    )

    # Return the logger instance
    return logging.getLogger(script_name)


# Get the script name (without the .py extension) to pass to the logging setup
script_name = os.path.splitext(os.path.basename(__file__))[0]

# Set up logging using the script name
logger = setup_logging(script_name)

# Now you can use the logger as usual
logger.info("This is an informational message.")

@timing_decorator
def retrieve_tags():
    """
    Retrieves tags and subcategories from CSV files and returns them as formatted strings.
    """
    tags_string = "None"
    subcategory_string = "None"

    # Retrieve tags
    if os.path.exists(TAGS_LIST):
        tags_df = pd.read_csv(TAGS_LIST)
        if 'tags' in tags_df.columns:
            tags_list = tags_df['tags'].dropna().tolist()
            tags_string = ", ".join(tags_list) if tags_list else "None"

    # Retrieve subcategories
    if os.path.exists(CATEGORY_SUBCATEGORY_LIST):
        subcategory_df = pd.read_csv(CATEGORY_SUBCATEGORY_LIST)
        if 'category' in subcategory_df.columns and 'subcategory' in subcategory_df.columns:
            grouped = subcategory_df.groupby('category')['subcategory'].apply(
                lambda x: ",".join(x)).reset_index()
            subcategory_string = "\n".join(
                [f"{row['category']}: {row['subcategory']}" for _, row in grouped.iterrows()])
            subcategory_string = subcategory_string if subcategory_string else "None"

    return tags_string, subcategory_string


@timing_decorator
def extract_json(response):
    """
    Extract JSON from the API response using direct parsing and fallback to regex.
    """
    try:
        content = response.json()['choices'][0]['message']['content']
    except Exception as e:
        logger.error("Error parsing response JSON: %s", e)
        return {}
    try:
        result = json.loads(content)
        logger.info("Direct JSON parsing successful.")
        return result
    except json.JSONDecodeError:
        logger.warning(
            "Direct JSON parsing failed. Trying regex extraction...")
    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        json_str = match.group(0).strip()
        try:
            result = json.loads(json_str)
            logger.info("Regex JSON extraction successful.")
            return result
        except json.JSONDecodeError:
            logger.error("Regex JSON extraction also failed.")
    logger.error("No valid JSON found.")
    return {}


def save_request_data(conversation_id, request_type, prompt, context, system_content, response, token_counts):
    """
    Save request and response data to a JSON file for logging/debugging purposes.
    """
    os.makedirs("requests", exist_ok=True)
    timestamp = int(time.time())
    filename = f"requests/{conversation_id}_{request_type}_{timestamp}.json"
    data = {
        "conversation_id": conversation_id,
        "request_type": request_type,
        "timestamp": timestamp,
        "prompt": prompt,
        "context": context,
        "system_content": system_content,
        "response": response,
        "token_counts": token_counts
    }
    try:
        with open(filename, 'w') as f:
            json.dump(data, f, indent=2)
        logger.info("Request saved to %s", filename)
    except Exception as e:
        logger.error("Error saving request data: %s", e)


def build_api_request(prompt: str, user_context: str, existing_subcategories: str, existing_tags: str, system_overview: str) -> dict:
    """
    Build the API request payload.
    """
    system_content = (
        "You are an AI specializing in location classification. "
        f"Existing subcategories: {existing_subcategories}. "
        f"Existing descriptive tags: {existing_tags}. "
        f"User conversation history: {user_context}. "
        f"{system_overview} "
        "Analyze the user's prompt to determine which subcategories (only subcategory names, not categories) it fits into and which descriptive tags apply. "
        "Return the matching subcategories and descriptive tags. "
        "If the prompt does not exactly match any existing tag, generate new ones that better capture its essence. "
        "Return both subcategories and descriptive tags. "
        "If multiple valid subcategories exist and the intent is unclear, return a clarification question."
    )
    return {
        "model": "llama3.1-70b",
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": f"Analyze this prompt: '{prompt}'"}
        ],
        "functions": [
            {
                "name": "extract_location_info",
                "description": (
                    "Extract the most relevant subcategories and descriptive tags from the user's prompt based on the provided context. "
                    "For subcategories, compare the prompt with the existing list and return the relevant matches. "
                    "For descriptive tags, do the same by returning matched tags or generating new descriptive words that capture the location's nuances. "
                    "Ensure both subcategories and tags are unique, non-redundant, and appropriately capture the nuances of the location described in the prompt. "
                    "If multiple subcategories are found and the intent is unclear, generate a clarification question."
                ),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "subcategories": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of top 3 matching subcategories."
                        },
                        "tags": {
                            "type": "object",
                            "properties": {
                                "existed": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "Top 3 descriptive tags that match existing ones."
                                },
                                "new": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                    "description": "New descriptive tags generated from the prompt."
                                }
                            },
                            "required": ["existed", "new"]
                        },
                        "clarification": {
                            "type": "object",
                            "properties": {
                                "needed": {
                                    "type": "boolean",
                                    "description": "True if clarification is needed, False if the classification is certain."
                                },
                                "question": {
                                    "type": "string",
                                    "description": "Clarification question to ask the user if needed."
                                }
                            },
                            "required": ["needed", "question"]
                        }
                    },
                    "required": ["subcategories", "tags", "clarification"]
                }
            }
        ],
        "function_call": "extract_location_info",
        "max_tokens": 5000,
        "temperature": 0.2,
    }


@timing_decorator
def llm_api(prompt: str, user_context: Optional[List[str]] = None,
            conversation_id: Optional[str] = None,
            history_manager: Optional[HistoryManager] = None) -> LLMResponse:
    """
    LLM API function that prepares the request, sends it to the LLAMA_API, processes the response,
    and saves the complete interaction to history.
    """

    existing_tags_str, existing_subcategories_str = retrieve_tags()
    user_history = "\n".join(user_context) if user_context else "None"
    system_overview = ""  # Add additional system instructions if needed

    api_request_json = build_api_request(
        prompt, user_history, existing_subcategories_str, existing_tags_str, system_overview)
    input_tokens = count_tokens(
        api_request_json["messages"][0]["content"]) + count_tokens(prompt)
    response = LLAMA_API.run(api_request_json)
    response_data = response.json()
    try:
        response_content = response_data['choices'][0]['message']['content']
    except Exception as e:
        logger.error("Error extracting response content: %s", e)
        response_content = ""
    output_tokens = count_tokens(response_content)
    total_tokens = input_tokens + output_tokens
    token_counts = {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": total_tokens
    }

    parsed_json = extract_json(response)
    if parsed_json:
        clarification_question = parsed_json.get('clarification', {}).get(
            'question') if parsed_json.get('clarification', {}).get('needed') else None
        categories = parsed_json.get('subcategories', [])
        tags = parsed_json.get('tags', {}).get('existed', [])
        result = {
            "clarification": clarification_question,
            "categories": categories,
            "tags": tags
        }
    else:
        result = {"error": "Failed to extract JSON"}

    request_data = {
        "request_type": "llm_classification",
        "timestamp": int(time.time()),
        "prompt": prompt,
        "context": user_context,
        "system_content": api_request_json["messages"][0]["content"],
        "api_request": api_request_json,
        "api_response": response_data,
        "token_counts": token_counts
    }
    history_manager.add_llm_interaction(
        conversation_id=conversation_id,
        prompt=prompt,
        response=result,
        request_data=request_data
    )
    save_request_data(conversation_id,
                      "llm_classification",
                      prompt, user_context,
                      api_request_json["messages"][0]["content"],
                      response_data,
                      token_counts)
    return LLMResponse(result)
get_location_advice.py:
from typing import List, Optional, Dict
import re
import json
import numpy as np
import os
import uuid
import time
import logging
from config import LLAMA_API
from src.utils import timing_decorator, count_tokens
from src.history_manager import HistoryManager
from src.data_types import TopCandidates, LocationAdviceResponse

import os
import logging
from datetime import datetime


def setup_logging(script_name: str) -> logging.Logger:
    """
    Set up logging to create a unique log file based on the script name and timestamp.

    Parameters:
    - script_name: Name of the script (used to create the log folder).

    Returns:
    - logger: Configured logger instance.
    """
    # Create the log directory based on the script name
    log_directory = f'logs/{script_name}'
    os.makedirs(log_directory, exist_ok=True)

    # Generate a unique log filename based on the current timestamp
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    log_filename = os.path.join(log_directory, f"log_{timestamp}.log")

    # Set up logging to both the console and the log file
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),  # Log to the generated log file
            logging.StreamHandler()  # Also log to the console
        ]
    )

    # Return the logger instance
    return logging.getLogger(script_name)


# Get the script name (without the .py extension) to pass to the logging setup
script_name = os.path.splitext(os.path.basename(__file__))[0]

# Set up logging using the script name
logger = setup_logging(script_name)

# Now you can use the logger as usual
logger.info("This is an informational message.")


@timing_decorator
def save_request_data(conversation_id, request_type, prompt, context, system_content, response, token_counts, top_candidates=None):
    """
    Save the full request/response data to a JSON file.
    """
    os.makedirs("requests", exist_ok=True)
    timestamp = int(time.time())
    filename = f"requests/{conversation_id}_{request_type}_{timestamp}.json"

    data = {
        "conversation_id": conversation_id,
        "request_type": request_type,
        "timestamp": timestamp,
        "prompt": prompt,
        "context": context,
        "system_content": system_content,
        "response": response,
        "token_counts": token_counts
    }
    if top_candidates:
        data["top_candidates"] = top_candidates

    try:
        with open(filename, 'w') as f:
            json.dump(data, f, default=str, indent=2)
        logger.info("Request saved to %s", filename)
    except Exception as e:
        logger.error("Error saving request data: %s", e)


@timing_decorator
def format_top_candidates(top_candidates: TopCandidates) -> str:
    """
    Format top candidate points of interest into a readable string.
    Handles numpy types and None values properly.
    """
    lines = []
    for mode, candidates in top_candidates.items():
        lines.append(f"{mode.capitalize()} Mode:")
        if candidates and len(candidates) > 0:
            for poi in candidates:
                details = [f"Mode: {mode.capitalize()}"]
                for key, value in poi.items():
                    # Convert numpy types to Python native types
                    if isinstance(value, np.generic):
                        value = value.item()  # Replaced np.asscalar with .item()
                    if value is None or (isinstance(value, float) and np.isnan(value)):
                        continue
                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if isinstance(sub_value, np.generic):
                                sub_value = sub_value.item()  # Replaced np.asscalar with .item()
                            if sub_value is None or (isinstance(sub_value, float) and np.isnan(sub_value)):
                                continue
                            details.append(
                                f"{sub_key.capitalize()}: {sub_value}")
                    else:
                        details.append(f"{key.capitalize()}: {value}")
                lines.append("\n".join(details))
        else:
            lines.append(
                f"No locations found within the specified route distance for {mode} mode.")
    return "\n\n".join(lines) if lines else "No location data available."


@timing_decorator
def extract_location_json(response):
    """Extract JSON from location advice response using multiple methods"""
    try:
        content = response.json()['choices'][0]['message']['content']
    except Exception as e:
        logger.error("Error extracting response content: %s", e)
        return {}

    try:
        result = json.loads(content)
        logger.info("Direct JSON parsing successful.")
        return result
    except json.JSONDecodeError:
        logger.warning(
            "Direct JSON parsing failed. Trying regex extraction...")

    match = re.search(r'\{.*\}', content, re.DOTALL)
    if match:
        json_str = match.group(0).strip()
        try:
            result = json.loads(json_str)
            logger.info("Regex JSON extraction successful.")
            return result
        except json.JSONDecodeError:
            logger.error("Regex JSON extraction also failed.")

    logger.error("No valid JSON found in response content")
    return {}


def build_location_request(prompt: str, context_text: str, user_history: str,
                           latitude: float, longitude: float, search_radius: int) -> dict:
    """Build the location advice API request payload"""
    system_content = (
        "You are a friendly location recommendation assistant. "
        f"User coordinates: ({latitude}, {longitude}), Search radius: {search_radius}m\n"
        f"Conversation history:\n{user_history}\n\n"
        f"Context data:\n{context_text}\n\n"
        "Provide specific recommendations with details from the context. "
        "Include addresses and directions when available."
    )

    return {
        "model": "llama3.1-70b",
        "messages": [
            {"role": "system", "content": system_content},
            {"role": "user", "content": prompt}
        ],
        "functions": [
            {
                "name": "analyze_location_request",
                "description": (
                    "Determines if the prompt is a continuation of the conversation or a new request. "
                    "If it's a continuation, return continuation: true. "
                    "If it's not, return continuation: false and generate a response based on context and history "
                    "to answer the user's prompt, including details like address, coordinates, and directions."
                ),
                "parameters": {
                    "type": "object",
                    "properties": {
                        "continuation": {
                            "type": "boolean",
                            "description": "True if the query is a continuation, False if it's a new request."
                        },
                        "response": {
                            "type": "string",
                            "description": (
                                "A detailed response using the provided context to answer the user's prompt. "
                                "Include address, coordinates, and directions where available."
                            )
                        }
                    },
                    "required": ["continuation", "response"]
                }
            }
        ],
        "function_call": "analyze_location_request",
        "max_tokens": 7000,
        "temperature": 0.7
    }


@timing_decorator
def get_location_advice(prompt: str, history: List[str], top_candidates: TopCandidates,
                        latitude: float, longitude: float, search_radius: int,
                        conversation_id: Optional[str] = None,
                        history_manager: Optional[HistoryManager] = None) -> LocationAdviceResponse:
    """Main function to get location advice with structured response handling"""
    # Initialize conversation and history
    conversation_id = conversation_id or str(uuid.uuid4())
    history_manager = history_manager or HistoryManager()

    # Format context and history
    formatted_candidates = format_top_candidates(top_candidates)
    user_history = "\n".join(
        history) if history else "No previous conversation"

    # Build API request
    api_request = build_location_request(
        prompt, formatted_candidates, user_history,
        latitude, longitude, search_radius
    )

    # Track token usage
    input_tokens = count_tokens(
        api_request["messages"][0]["content"]) + count_tokens(prompt)

    try:
        # Execute API call
        response = LLAMA_API.run(api_request)
        response_data = response.json()
        response_content = response_data.get('choices', [{}])[
            0].get('message', {}).get('content', '')
        output_tokens = count_tokens(response_content)

        # Process response
        result = extract_location_json(response)

    except Exception as e:
        logger.error("API call failed: %s", e)
        result = LocationAdviceResponse(
            response="I couldn't process your request properly.",
            error=str(e)
        )
        output_tokens = 0

    # Save interaction
    token_counts = {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens
    }

    request_data = {
        "request_type": "location_advice",
        "timestamp": int(time.time()),
        "prompt": prompt,
        "context": user_history,
        "system_content": api_request["messages"][0]["content"],
        "api_request": api_request,
        "api_response": response_data,
        "token_counts": token_counts
    }

    # In get_location_advice.py, fix the history recording:
    history_manager.add_llm_interaction(
        conversation_id=conversation_id,
        prompt=prompt,
        response=result,  # Use the actual result, not request_data
        request_data=request_data,
        top_candidates=top_candidates  # Add this to ensure top_candidates are saved
    )
    save_request_data(
        conversation_id,
        "location_advice",
        prompt,
        user_history,
        api_request["messages"][0]["content"],
        response_data,
        token_counts
    )
    return LocationAdviceResponse(
        continuation=result.get("continuation"),
        response=result.get("response")
    )


there is a problem with the history recording as it saving same prompts and responses twice and even trice