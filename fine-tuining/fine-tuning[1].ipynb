{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ersan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForCausalLM,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorForLanguageModeling,\n",
    "#     TrainerCallback,\n",
    "# )\n",
    "# import torch\n",
    "# import evaluate\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # 1. MODEL AND DEVICE SETUP\n",
    "# # =============================================================================\n",
    "# # We fine-tune the Meta‑Llama model available on Hugging Face.\n",
    "# model_checkpoint = \"meta-llama/Llama-3.2-1B\"  # Model name on Hugging Face Hub\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load the tokenizer and model.\n",
    "# # (Note: Depending on your installation and the model repo, you might need to pass\n",
    "# #  additional arguments such as `trust_remote_code=True`.)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"./local_llama3\"  # Use the local path if downloaded\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_checkpoint, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# # Move model to device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "# import bitsandbytes as bnb\n",
    "\n",
    "# # Set model path (use local directory if downloaded)\n",
    "# model_checkpoint = \"./local_llama3\"  # Path to the downloaded model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "\n",
    "# # Load model with quantization using `bitsandbytes`\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_checkpoint,\n",
    "#     torch_dtype=torch.float16,  # For reduced precision (faster, less memory)\n",
    "#     device_map=\"auto\",          # Automatically place the model on available devices\n",
    "#     # 8-bit quantization for model weights (optional)\n",
    "#     load_in_8bit=True,\n",
    "#     quantization_config=bnb.quantization.QuantizationConfig(\n",
    "#         load_in_4bit=True       # 4-bit quantization (optional)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Move model to device\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.quantized.supported_engines)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Model yolu\n",
    "model_checkpoint = \"C:/Users/ersan/Desktop/mobile/local_llama3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU kontrolü\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Tokenizer ve modeli yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Test giriş metni\n",
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate output with attention_mask\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)  # Attention mask ekle\n",
    "output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=50)\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING\n",
    "# =============================================================================\n",
    "# We assume your data is stored in JSON files with the following keys:\n",
    "#    \"input_text\"  : user query\n",
    "#    \"target_text\" : JSON answer\n",
    "# Provide paths to your training, evaluation, and test files.\n",
    "data_files = {\n",
    "    \"train\": \"../data2/train_data.json\",         # Path to your training data\n",
    "    \"validation\": \"../data2/eval_data.json\",       # Path to your validation data\n",
    "    # Path to your test data (for inference)\n",
    "    \"test\": \"../data2/test_data.json\",\n",
    "}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. PREPROCESSING FUNCTION\n",
    "# =============================================================================\n",
    "# For causal LM fine-tuning, we combine the input query and its corresponding answer\n",
    "# into one text. A delimiter (\"\\nAnswer: \") is used to signal the beginning of the answer.\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    texts = []\n",
    "    for inp, tgt in zip(examples[\"input_text\"], examples[\"target_text\"]):\n",
    "        full_text = inp.strip() + \"\\nAnswer: \" + tgt.strip() + tokenizer.eos_token\n",
    "        texts.append(full_text)\n",
    "    # Tokenize the full text. We use truncation and a max_length; adjust as needed.\n",
    "    tokenized = tokenizer(texts, truncation=True, max_length=1024)\n",
    "    # For causal LM, our labels are the same as the input IDs.\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to all splits (train/validation).\n",
    "# We remove the original columns as they are no longer needed for training.\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. DATA COLLATOR\n",
    "# =============================================================================\n",
    "# The data collator dynamically pads each batch to the longest sequence.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TRAINING ARGUMENTS (ADVANCED)\n",
    "# =============================================================================\n",
    "# Here we set various advanced training options:\n",
    "#   - Mixed precision (fp16) for faster training.\n",
    "#   - Gradient accumulation to simulate a larger batch size.\n",
    "#   - Gradient clipping via max_grad_norm.\n",
    "training_args = TrainingArguments(\n",
    "    # Directory to store checkpoints and logs.\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    # evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch.\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,        # Adjust based on your GPU memory.\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,        # Effective batch size = 2*8 = 16.\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,                          # Enable mixed precision training.\n",
    "    max_grad_norm=1.0,                  # Clip gradients to this norm.\n",
    "    # Use model.generate() during evaluation.\n",
    "    # predict_with_generate=True,\n",
    "    # Change to \"tensorboard\"/\"wandb\" for advanced logging.\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CUSTOM CALLBACK: GRADIENT LOGGING\n",
    "# =============================================================================\n",
    "# This callback computes and logs the L2 norm of gradients at the end of each step.\n",
    "\n",
    "\n",
    "class GradientLoggingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        total_norm = 0.0\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        print(f\"[Step {state.global_step}] Gradient Norm: {total_norm:.4f}\")\n",
    "\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# =============================================================================\n",
    "# 7. EVALUATION METRIC: COMPUTE ROUGE\n",
    "# =============================================================================\n",
    "# We use ROUGE (via the evaluate library) to compare generated outputs to the targets.\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Replace -100 (masked positions) in the labels with the pad token id.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # Format ROUGE scores: here we report F-measure * 100.\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "# =============================================================================\n",
    "# 8. GRADIENT TEST FUNCTION\n",
    "# =============================================================================\n",
    "# Before training, we perform a gradient test on a single sample to verify that gradients flow.\n",
    "\n",
    "\n",
    "def gradient_test():\n",
    "    sample = tokenized_datasets[\"train\"][0]\n",
    "    # Prepare tensors and add a batch dimension.\n",
    "    input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(\n",
    "        sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(device)\n",
    "    model.train()\n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    grad_norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm += param.grad.norm().item() ** 2\n",
    "    grad_norm = grad_norm ** 0.5\n",
    "    print(\n",
    "        f\"Gradient Test -> Loss: {loss.item():.4f}, Gradient Norm: {grad_norm:.4f}\")\n",
    "\n",
    "\n",
    "# Run the gradient test.\n",
    "gradient_test()\n",
    "\n",
    "# =============================================================================\n",
    "# 9. INITIALIZE THE TRAINER\n",
    "# =============================================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[GradientLoggingCallback()],\n",
    "    # prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10. TRAINING\n",
    "# =============================================================================\n",
    "# Start fine-tuning. Checkpoints, logs, and evaluations will be saved per the training_args.\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer.\n",
    "trainer.save_model(\"./llama_finetuned_final\")\n",
    "tokenizer.save_pretrained(\"./llama_finetuned_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 11. TESTING / INFERENCE\n",
    "# =============================================================================\n",
    "# Now we use the (raw) test set for generating answers.\n",
    "# For each test sample, we only feed the \"input_text\" (the query) to the model,\n",
    "# and then generate a completion which should ideally be the JSON answer.\n",
    "\n",
    "\n",
    "def generate_answer(example):\n",
    "    prompt = example[\"input_text\"].strip() + \"\\nAnswer: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Generate output using beam search for higher-quality generations.\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    # If the generated text repeats the prompt, remove it.\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set.\n",
    "test_dataset = raw_datasets[\"test\"]\n",
    "references = []\n",
    "predictions = []\n",
    "print(\"\\n--- Generating answers on test set ---\")\n",
    "for example in tqdm(test_dataset, desc=\"Testing\"):\n",
    "    pred = generate_answer(example)\n",
    "    predictions.append(pred)\n",
    "    references.append(example[\"target_text\"])\n",
    "\n",
    "# Compute ROUGE scores on the test set.\n",
    "test_rouge = rouge_metric.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "test_rouge = {key: value.mid.fmeasure *\n",
    "              100 for key, value in test_rouge.items()}\n",
    "print(\"\\nTest ROUGE Scores:\")\n",
    "for k, v in test_rouge.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# Print a few examples for manual inspection.\n",
    "print(\"\\nSample Generations:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nInput: {test_dataset[i]['input_text']}\")\n",
    "    print(f\"Target: {test_dataset[i]['target_text']}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
