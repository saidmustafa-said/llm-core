{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', 'onednn', 'x86', 'fbgemm']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ersan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to find CUDA.\n",
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI is now. It’s not a question of if, but when. And in this digital age, it’s time to start preparing your business for the future of AI. With the right strategies and tools in place, you can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2348/2348 [00:00<00:00, 2768.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Test -> Loss: 9.5910, Gradient Norm: 1130.0015\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Model ve cihaz ayarları\n",
    "model_checkpoint = \"C:/Users/ersan/Desktop/mobile/local_llama3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Tokenizer ve model yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Test\n",
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# Veri yükleme\n",
    "data_files = {\n",
    "    \"train\": \"../data2/train_data.json\",\n",
    "    \"validation\": \"../data2/eval_data.json\",\n",
    "    \"test\": \"../data2/test_data.json\",\n",
    "}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_function(examples):\n",
    "    texts = [inp.strip() + \"\\nAnswer: \" + tgt.strip() + tokenizer.eos_token\n",
    "             for inp, tgt in zip(examples[\"input_text\"], examples[\"target_text\"])]\n",
    "    tokenized = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Eğitim argümanları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Gradient logging callback\n",
    "class GradientLoggingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        total_norm = sum(param.grad.detach().data.norm(2).item() ** 2\n",
    "                         for param in model.parameters() if param.grad is not None) ** 0.5\n",
    "        print(f\"[Step {state.global_step}] Gradient Norm: {total_norm:.4f}\")\n",
    "\n",
    "# ROUGE metrik hesaplama\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "# Gradient testi\n",
    "def gradient_test():\n",
    "    sample = tokenized_datasets[\"train\"][0]\n",
    "    input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(device)\n",
    "    model.train()\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    grad_norm = sum(param.grad.norm().item() ** 2 for param in model.parameters() if param.grad is not None) ** 0.5\n",
    "    print(f\"Gradient Test -> Loss: {loss.item():.4f}, Gradient Norm: {grad_norm:.4f}\")\n",
    "\n",
    "gradient_test()\n",
    "\n",
    "# Trainer tanımla ve eğitimi başlat\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[GradientLoggingCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./llama_finetuned_final\")\n",
    "tokenizer.save_pretrained(\"./llama_finetuned_final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
