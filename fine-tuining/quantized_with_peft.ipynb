{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Model ve cihaz ayarları\n",
    "model_checkpoint = \"C:/Users/ersan/Desktop/mobile/local_llama3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Tokenizer yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantized model yükle\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    "    # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# PEFT ve LoRA yapılandırması\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank of the low-rank approximation\n",
    "    lora_alpha=16,  # scaling factor for the LoRA layers\n",
    "    lora_dropout=0.1,  # dropout for LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",  # task type, for CausalLM models\n",
    ")\n",
    "\n",
    "# PEFT modelini LoRA ile adapte et\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Test\n",
    "input_text = \"The future of AI is\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "output_ids = peft_model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# Veri yükleme\n",
    "data_files = {\n",
    "    \"train\": \"../data_less/train_data.json\",\n",
    "    \"validation\": \"../data_less/eval_data.json\",\n",
    "    \"test\": \"../data_less/test_data.json\",\n",
    "}\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Ön işleme fonksiyonu\n",
    "def preprocess_function(examples):\n",
    "    texts = [inp.strip() + \"\\nAnswer: \" + tgt.strip() + tokenizer.eos_token\n",
    "             for inp, tgt in zip(examples[\"input_text\"], examples[\"target_text\"])]\n",
    "    tokenized = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Eğitim argümanları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Gradient logging callback\n",
    "class GradientLoggingCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        total_norm = sum(param.grad.detach().data.norm(2).item() ** 2\n",
    "                         for param in model.parameters() if param.grad is not None) ** 0.5\n",
    "        print(f\"[Step {state.global_step}] Gradient Norm: {total_norm:.4f}\")\n",
    "\n",
    "# ROUGE metrik hesaplama\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#     result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "#     return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits to token ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Replace -100 in the labels as needed\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds, \n",
    "        references=decoded_labels, \n",
    "        use_stemmer=True\n",
    "    )\n",
    "    # return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "# Trainer tanımla ve eğitimi başlat\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[GradientLoggingCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./llama_peft_finetuned_final\")\n",
    "tokenizer.save_pretrained(\"./llama_peft_finetuned_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input Prompt ===\n",
      "I am located in Türkiye/İstanbul. I would like to learn more about a place called 'Opet'.\n",
      "Please provide the details in JSON format as per the following structure:\n",
      "{\"name\": ..., \"description\": ..., \"location\": ..., \"coordinates\": {\"latitude\": ..., \"longitude\": ...}, \"tags\": [...]}\n",
      "Answer:\n",
      "\n",
      "=== Generated Output ===\n",
      "I am located in Türkiye/İstanbul. I would like to learn more about a place called 'Opet'.\n",
      "Please provide the details in JSON format as per the following structure:\n",
      "{\"name\":..., \"description\":..., \"location\":..., \"coordinates\": {\"latitude\":..., \"longitude\":...}, \"tags\": [...]}\n",
      "Answer: { \"name\": \"Opet\", \"description\": \"An ancient Roman public bath and gymnasium in the city of Ephesus, Turkey\", \"location\": \"Ephesus, Turkey\", \"coordinates\": {\"latitude\": 38.3396, \"longitude\": 27.9801}, \"tags\": [\"Bath\", \"Gymnasium\", \"Roman\", \"Public\", \"Ancient\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define model path (adjust based on your saved directory)\n",
    "model_path = \"./llama_peft_finetuned_final\"\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "\n",
    "# Load PEFT model (LoRA-adapted)\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "peft_model.to(device)  # Move model to GPU if available\n",
    "\n",
    "def test_model_prompt(model, tokenizer, device, prompt_text):\n",
    "    print(\"=== Input Prompt ===\")\n",
    "    print(prompt_text)\n",
    "    \n",
    "    # Tokenize with attention mask\n",
    "    encoding = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "    \n",
    "    # Generate output with sampling enabled\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=150,\n",
    "            do_sample=True,      # Enable sampling to allow creative output\n",
    "            temperature=0.7,      # Adjust temperature for output diversity\n",
    "            top_p=0.9,            # Use nucleus sampling\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n=== Generated Output ===\")\n",
    "    print(generated_text)\n",
    "\n",
    "# Example test prompt with clear JSON formatting instructions\n",
    "prompt_text = (\n",
    "    \"I am located in Türkiye/İstanbul. I would like to learn more about a place called 'Opet'.\\n\"\n",
    "    \"Please provide the details in JSON format as per the following structure:\\n\"\n",
    "    \"{\\\"name\\\": ..., \\\"description\\\": ..., \\\"location\\\": ..., \\\"coordinates\\\": {\\\"latitude\\\": ..., \\\"longitude\\\": ...}, \\\"tags\\\": [...]}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "# Run the test\n",
    "test_model_prompt(peft_model, tokenizer, device, prompt_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
